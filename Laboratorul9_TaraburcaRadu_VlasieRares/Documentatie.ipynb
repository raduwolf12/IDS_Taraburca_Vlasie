{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laborator 9 - Modele de regresie\n",
    "<br>\n",
    "Autori:\n",
    "Taraburca Radu,\n",
    "Vlasie Rares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Regresia liniară"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regresia liniară este utilizată pentru a găsi relația liniară între o valoare target și unul sau mai mulți predictori. Există două tipuri de regresie liniară: simpla și multipla."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Regresia liniară simplă</b>  este utilă pentru găsirea unei relații între două variabile continue. Una este predictoare sau variabilă independentă, iar alta este variabilă de răspuns sau dependentă. Se caută relația statistică, dar nu relația deterministă."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Regresia liniară multiplă (MLR)</b>, cunoscută și sub denumirea de regresie multiplă, este o tehnică statistică care folosește mai multe variabile explicative pentru a prezice rezultatul unei variabile de răspuns. Scopul regresiei liniare multiple (MLR) este modelarea relației liniare între variabilele explicative (independente) și variabila de răspuns (dependentă)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class sklearn.linear_model.LinearRegression(*, fit_intercept=True, normalize=False, copy_X=True, n_jobs=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parametrii:\n",
    "\n",
    "- fit_intercept - dacă se calculează interceptul pentru acest model. Dacă este setat la False, interceptarea nu va fi utilizată în calcule (de exemplu, datele vor fi deja centrate), valoare booleana, default va avea valoare True.\n",
    "\n",
    "- normalize-Acest parametru este ignorat atunci când fit_intercept este setat la False. Dacă este adevărat, regresorii X vor fi normalizați înainte de regresie prin scăderea mediei și împărțirea după norma l2. Valoare booleana, default va avea valoare False. \n",
    "\n",
    "- copy_X-Dacă este adevărat, X va fi copiat; altfel, va fi suprascris. Valoare booleana, default va avea valoare True. \n",
    "\n",
    "- n_jobs-numarul de job-uri pe care procesorul le va face, valoarea -1 inseamna folosirea tuturor resurselor. Acesta are o valoare intreaga sau chiar None."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![liniarRegresion.png](liniarRegresion.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bibliografie\n",
    "- https://en.wikipedia.org/wiki/Linear_regression#cite_note-Freedman09-1\n",
    "<br>\n",
    "- https://www.investopedia.com/terms/m/mlr.asp<br>\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html<br>\n",
    "- https://towardsdatascience.com/linear-regression-detailed-view-ea73175f6e86"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) KNeighborsRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNeighborsRegressor este o metodă non-parametrică utilizată pentru regresie. Intrarea constă din cele mai apropiate k exemple de instruire din spațiul de funcții. Rezultatul este valoarea proprietății pentru obiect. Această valoare este media valorilor a k vecini apropiați."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNeighborsRegressor este un tip de învățare bazată pe instanță sau învățare leneșă, unde funcția este aproximată doar local și toată calcularea este amânată până la evaluarea funcției."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class sklearn.neighbors.KNeighborsRegressor(n_neighbors=5, *, weights='uniform', algorithm='auto', leaf_size=30, p=2, metric='minkowski', metric_params=None, n_jobs=None, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parametrii:\n",
    "- n_neighbors : int, optional (default = 5): Numărul de vecini care vor fi utilizați în mod implicit pentru interogările k_neighbors.\n",
    "- weights: string (default='uniform'): funcția de greutate utilizată în predicție. Valorile posibile:\n",
    "\n",
    "- \"uniform\": greutăți uniforme. Toate punctele din fiecare cartier sunt ponderate în mod egal; \n",
    "\n",
    "- \"distance\": punctele de greutate prin inversul distanței lor. în acest caz, vecinii mai apropiați ai unui punct de interogare vor avea o influență mai mare decât vecinii aflați mai departe.\n",
    "- algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, optional. Algoritmul folosit pentru a calcula cei mai apropiați vecini:\n",
    "\n",
    "- \"ball_tree\" va folosi BallTree; \n",
    "\n",
    "- 'kd_tree' va folosi KDtree;\n",
    "\n",
    "- \"brute\" va folosi o căutare de forțe brute; \n",
    "\n",
    "- \"auto\" va încerca să decidă cel mai potrivit algoritm pe baza valorilor transmise pentru a se potrivi cu metoda.\n",
    "- leaf_size : int, optional (default = 30). Folosit la BallTree sau KDTree. Acest lucru poate afecta viteza construcției și interogarea, precum și memoria necesară pentru stocare. Valoarea optimă depinde de natura problemei.\n",
    "- metric : string or DistanceMetric object (default='minkowski'). Metrica de distanță care trebuie utilizată pentru arbore. Valoarea implicită este minkowski, iar cu p = 2 este echivalentă cu ecuația standard Euclidiană.\n",
    "- p : int, optional (default = 2). Atunci când p = 1, aceasta este echivalentă cu utilizarea manhattan_distance (l1) și a duratei euclidiene (l2) pentru p = 2. Pentru p arbitrar, se utilizează minkowski_distance (l_p).\n",
    "- metric_params : dict, optional (default = None).  Cuvinte cheie suplimentare pentru funcția de metrică."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![knn.webp](knn.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bibliografie\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html\n",
    "<br>\n",
    "-  https://kite.com/python/docs/sklearn.neighbors.KNeighborsRegressor\n",
    "- https://www.analyticsvidhya.com/blog/2018/08/k-nearest-neighbor-introduction-regression-python/\n",
    "- https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class sklearn.tree.DecisionTreeRegressor(*, criterion='mse', splitter='best', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, presort='deprecated', ccp_alpha=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arborii de decizie (DT) sunt o metodă de învățare supravegheată non-parametrică folosită pentru clasificare și regresie. Scopul este de a crea un model care prezice valoarea unei variabile țintă, învățând reguli simple de decizie deduse din caracteristicile datelor.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parametrii:\n",
    "- criterion : string, optional (default=\"mse\") Funcția de măsurare a calității unei divizări. Singurul criteriu acceptat este \"mse\" pentru eroarea medie pătrată, care este egală cu reducerea varianței ca criteriu de selecție a caracteristicilor.\n",
    "\n",
    "\n",
    "- splitter : string, optional (default=\"best\") Strategia folosită pentru alegerea divizării la fiecare nod. Strategiile acceptate sunt \"best\" pentru a alege cea mai bună divizare și \"random\" pentru a alege cea mai bună diviziune aleatorie.\n",
    "\n",
    "\n",
    "- max_features : int, float, string or None, optional (default=None) Numărul de caracteristici care trebuie luate în considerare atunci când căutați cea mai bună împărțire:\n",
    "    *Dacă \"int\", atunci luați în considerare caracteristicile max_features la fiecare split.\n",
    "\n",
    "    *Dacă \"float\", atunci max_features este un procent și caracteristicile int (max_features * n_features) sunt luate în considerare la fiecare split.\n",
    "\n",
    "    *Dacă \"auto\", atunci max_features = n_features.\n",
    "\n",
    "    *Dacă \"sqrt\", atunci max_features = sqrt (n_features).\n",
    "\n",
    "    *Dacă \"log2\", atunci max_features = log2 (n_features).\n",
    "\n",
    "    *Dacă nu există, atunci max_features = n_features.\n",
    "\n",
    "\n",
    "- max_depth : int or None, optional (default=None)Adâncimea maximă a arborelui. Dacă nu există, atunci nodurile sunt extinse până când toate frunzele sunt pure sau până când toate frunzele conțin mai puțin decât probele min_samples_split. Ignorat dacă max_leaf_nodes nu este None.\n",
    "\n",
    "\n",
    "\n",
    "- min_samples_split : int, optional (default=2) Numărul minim de eșantioane necesare pentru divizarea unui nod intern.\n",
    "\n",
    "\n",
    "\n",
    "- min_samples_leaf : int, optional (default=1) Numărul minim de eșantioane necesare la un nod frunza.\n",
    "\n",
    "\n",
    "\n",
    "- min_weight_fraction_leaf : float, optional (default=0.) Fracțiunea minimă ponderată a probelor de intrare care trebuie să fie la un nod de frunze.\n",
    "\n",
    "\n",
    "\n",
    "- max_leaf_nodes : int or None, optional (default=None) Dacă \"none\" există un număr nelimitat de noduri frunze. Dacă nu este \"none\" atunci max_depth va fi ignorat.\n",
    "\n",
    "\n",
    "\n",
    "- random_state : int, RandomState instance or None, optional (default=None) Dacă este \"int\", random_state este generatorul de numere aleatoare; Dacă este instanța \"RandomState\", random_state este generatorul de numere aleatoare; Dacă nu există, generatorul de numere aleatoare este instanța RandomState folosită de np.random\n",
    "\n",
    "\n",
    "\n",
    "- presort : bool, optional (default=False) Pentru setările implicite ale unui arbore de decizie pe seturi de date mari, setarea acestuia la \"true\" poate încetini procesul de antrenament. Atunci când utilizați un set de date mai mic sau o adâncime limitată, acest lucru poate accelera procesul de antrenament.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![dectree.png](dectree.png )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliografie:\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html\n",
    "- https://kite.com/python/docs/sklearn.tree.tree.DecisionTreeRegressor\n",
    "- https://scikit-learn.org/stable/modules/tree.html#tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) SVR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class sklearn.svm.SVR(*, kernel='rbf', degree=3, gamma='scale', coef0=0.0, tol=0.001, C=1.0, epsilon=0.1, shrinking=True, cache_size=200, verbose=False, max_iter=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metoda de clasificare a vectorului suport poate fi extinsă pentru a rezolva problemele de regresie. Această metodă se numește Regresiunea Vectorială de Suport (SVR).\n",
    "\n",
    "Modelul produs prin clasificarea vectorului de suport (așa cum este descris mai sus) depinde doar de un subset de date de instruire, deoarece funcția de costuri pentru construirea modelului nu are grijă de punctele de formare care depășesc marja. În mod analog, modelul produs de suport Vector Regression depinde doar de un subset de date de instruire, deoarece funcția de cost ignoră eșantioanele a căror predicție este aproape de ținta lor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parametrii:\n",
    "- C : float, optional (default=1.0) Parametrul de penalizare C al termenului de eroare.\n",
    " \n",
    " \n",
    "- epsilon : float, optional (default=0.1) Specifică epsilonul în care nu este asociată nici o penalitate în funcția de pierdere a setului de antrenare cu punctele prezise la o distanță epsilon față de valoarea reală.\n",
    "\n",
    "\n",
    "- kernel : string, optional (default='rbf') Trebuie să fie una dintre cele \"linear\", \"poli\", \"rbf\", \"sigmoid\", \"precomputed\" sau \"callable\". Dacă nu se dă niciunul, se va folosi \"rbf\". Dacă este dat un apel callable, acesta este folosit pentru a precompune matricea kernel-ului.\n",
    "\n",
    "\n",
    "- degree : int, optional (default=3) Gradul funcției kernelului polinomial (\"poli\"). Ignorat de toate celelalte kernel-uri.\n",
    "\n",
    "\n",
    "- gamma : float, optional (default='auto') Coeficientul kernel pentru \"rbf\", \"poly\" și \"sigmoid\". Dacă gama este \"auto\", atunci vor fi utilizate 1 / n_features.\n",
    "\n",
    "\n",
    "- coef0 : float, optional (default=0.0) Termen independent în funcția de kernel. Este semnificativă numai în \"poli\" și \"sigmoid\".\n",
    "\n",
    "\n",
    "- shrinking : boolean, optional (default=True)\n",
    "\n",
    "\n",
    "- tol : float, optional (default=1e-3)\n",
    "\n",
    "\n",
    "- cache_size : float, optional Dimensiunea memoriei cache a kernel (în MB).\n",
    "\n",
    "\n",
    "- verbose : bool, default: False\n",
    "\n",
    "\n",
    "- max_iter : int, optional (default=-1) Limita maximă a iterațiilor sau -1 pentru nici o limită."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![svm.png](svm.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliografie:\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html\n",
    "- https://scikit-learn.org/stable/modules/svm.html#svm-regression\n",
    "- https://scikit-learn.org/stable/auto_examples/svm/plot_svm_regression.html#sphx-glr-auto-examples-svm-plot-svm-regression-py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5) MLPRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class sklearn.neural_network.MLPRegressor(hidden_layer_sizes=(100, ), activation='relu', *, solver='adam', alpha=0.0001, batch_size='auto', learning_rate='constant', learning_rate_init=0.001, power_t=0.5, max_iter=200, shuffle=True, random_state=None, tol=0.0001, verbose=False, warm_start=False, momentum=0.9, nesterovs_momentum=True, early_stopping=False, validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08, n_iter_no_change=10, max_fun=15000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un perceptron multistrat (MLP) este o clasă de rețea neuronală artificială avansată. Un MLP este format din cel puțin trei straturi de noduri: un strat de intrare, un strat ascuns și un strat de ieșire. Cu excepția nodurilor de intrare, fiecare nod este un neuron care utilizează o funcție de activare neliniară. MLP utilizează o tehnică de învățare supravegheată numită backpropagation pentru instruire."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parametrii:\n",
    "- hidden_layer_sizes : tuple, length = n_layers - 2, default (100,).Elementul i reprezintă numărul de neuroni din stratul i.\n",
    "- activation : {‘identity’, ‘logistic’, ‘tanh’, ‘relu’}, default ‘relu’\n",
    "Funcția de activare pentru stratul ascuns.\n",
    "\n",
    "    - \"identity\", activare no-op, returnează f (x) = x \n",
    "    - \"logistic\", funcția sigmoid logistică, returnează f (x) = 1 / (1 + exp (-x)). \n",
    "    - \"tanh\", funcția tan hiperbolică, returnează f (x) = tanh (x). \n",
    "    - \"relu\", funcția liniară rectificată, returnează f (x) = max (0, x)\n",
    "\n",
    "- solver : {‘lbfgs’, ‘sgd’, ‘adam’}, default ‘adam’ Solver pentru optimizarea greutății.\n",
    "\n",
    "     - \"Lbfgs\" este un optimizator în familia metodelor cvasi-Newton.  \n",
    "     - 'Sgd' se referă la coborârea stochastică a gradientului. \n",
    "     - \"Adam\" se referă la un optimizator bazat pe gradient stochastic propus de Kingma, Diederik și Jimmy Ba\n",
    "\n",
    "- alpha : float, optional, default 0.0001\n",
    "Valoarea penalizării L2 (termenul de regularizare).\n",
    "\n",
    "- batch_size : int, optional, default ‘auto’\n",
    "Dimensiunea loturilor mini pentru optimizatori stochastici. Dacă solverul este \"lbfgs\", clasificatorul nu va folosi mini lot. Când este setat la \"auto\", batch_size = min (200, n_samples)\n",
    "\n",
    "- learning_rate : {‘constant’, ‘invscaling’, ‘adaptive’}, default ‘constant’\n",
    "Rata de învățare pentru actualizări în greutate.\n",
    "\n",
    "    - 'Constant' este o rată constantă de învățare dată de 'learning_rate_init'.\n",
    "\n",
    "    - ‘invscaling’ scade treptat rata de învățare learning_rate_ la fiecare pas de timp 't' folosind un exponent invers de scalare al 'power_t'. effective_learning_rate = learning_rate_init / pow (t, power_t)\n",
    "\n",
    "    - ‘adaptive’ menține rata de învățare constantă la \"learning_rate_init\" atâta timp cât pierderea de formare continuă să scadă.\n",
    "\n",
    "- power_t : double, optional, default 0.5 Exponentul pentru rata de invatare scalare inversa. Se folosește la actualizarea ratei efective de învățare când learning_rate este setat la 'int scaling'. Se folosește numai atunci când solver = 'sgd'.\n",
    "\n",
    "- max_iter : int, optional, default 200 Numărul maxim de iterații.Pentru solverii stochastice ('sgd', 'adam'), acest lucru determină numărul de de câte ori va fi folosit fiecare punct de date, nu numărul de pași de gradient.\n",
    "\n",
    "- random_state : int, RandomState instance or None, optional, default None Dacă instanța RandomState, random_state este generatorul de numere aleatoare; Dacă nu există, generatorul de numere aleatoare este instanța RandomState folosită de np.random."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![mlp.jpeg](mlp.jpeg) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliografie:\n",
    "- https://en.wikipedia.org/wiki/Multilayer_perceptron\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html\n",
    "- https://medium.com/@AI_with_Kain/understanding-of-multilayer-perceptron-mlp-8f179c4a135f"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
